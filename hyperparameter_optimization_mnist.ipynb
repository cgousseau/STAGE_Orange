{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PRE-PROCESSING\n",
    "\n",
    "## Import the data \n",
    "from keras.datasets import mnist\n",
    "(x_train, Y_train), (xtest, Ytest) = mnist.load_data()\n",
    "\n",
    "## Create training and validation set\n",
    "xtrain=x_train[:int(0.8*len(x_train)),:,:]\n",
    "Ytrain=Y_train[:int(0.8*len(x_train))]\n",
    "xval=x_train[int(0.8*len(x_train)):,:,:]\n",
    "Yval=Y_train[int(0.8*len(x_train)):]\n",
    "\n",
    "## Normalization\n",
    "xtrain = xtrain/255\n",
    "xval = xval/255\n",
    "xtest = xtest/255\n",
    "\n",
    "## Example of images\n",
    "for t in range(1,10):\n",
    "    plt.subplot(330+t)\n",
    "    plt.imshow(xtrain[t],cmap='Greys')\n",
    "plt.show()\n",
    "\n",
    "## Reshape the data\n",
    "xtrain=np.reshape(xtrain,(xtrain.shape[0],xtrain.shape[1],xtrain.shape[2],1))\n",
    "xval=np.reshape(xval,(xval.shape[0],xval.shape[1],xval.shape[2],1))\n",
    "xtest=np.reshape(xtest,(xtest.shape[0],xtest.shape[1],xtest.shape[2],1))\n",
    "\n",
    "## One-hot encoding the labels\n",
    "from keras.utils.np_utils import to_categorical\n",
    "ytrain = to_categorical(Ytrain)\n",
    "yval = to_categorical(Yval)\n",
    "ytest = to_categorical(Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL GENERATION\n",
    "\n",
    "## Hyperparameter generation\n",
    "\n",
    "# makeHyper generates a set of hyperparameters:\n",
    "# During the optimization process all the hyperparameters are normalized to range between 0 and 1\n",
    "# makeHyper makes the transformation between 0 and 1 and the actuel hyperparameter space:\n",
    "    # l2reg ranges in [1,1e-10] (float)\n",
    "    # dropout ranges in [0,1] (float)\n",
    "    # batchsize ranges in [1,100] (integer)\n",
    "    \n",
    "def makeHyper(hyper):\n",
    "    \n",
    "    # for PSO\n",
    "    l2reg = np.power(10,-10*hyper[0])\n",
    "    dropout = hyper[1]\n",
    "    batchsize = 1+int(99*hyper[2])\n",
    "    \n",
    "    # for TPE\n",
    "    #l2reg = np.power(10,-10*hyper['l2reg'])\n",
    "    #dropout = hyper['dropout']\n",
    "    #batchsize = 1+int(99*hyper['batchsize'])\n",
    "    \n",
    "    return([l2reg,dropout,batchsize])\n",
    "\n",
    "## Model generation\n",
    "\n",
    "# createModel creates a LeNet1-model given an array 'hyper'\n",
    "# 'hyper' is an array of normalized hyperparameters (float in [0,1])\n",
    "\n",
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "\n",
    "def createModel(hyper):\n",
    "        \n",
    "    [l2reg,dropout,batchsize]=makeHyper(hyper) # makes the transformation between [0,1] and the actuel hyperparameter space\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(4, kernel_size=(5, 5), strides=(1, 1), activation='tanh', input_shape=(28,28,1)))\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Conv2D(12, kernel_size=(5, 5), strides=(1, 1), activation='tanh'))\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(10, activation='softmax',kernel_regularizer=keras.regularizers.l2(l2reg)))\n",
    "    \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL EVALUATION\n",
    "def objective(hyper):\n",
    "    \n",
    "    model = createModel(hyper)\n",
    "    model.compile(optimizer = keras.optimizers.Adam(lr=0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    cb = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto', baseline=None, restore_best_weights=True)]   \n",
    "    \n",
    "    [l2reg,dropout,batchsize]=makeHyper(hyper)\n",
    "    print('l2reg = '+str(l2reg)+' | dropout = '+str(dropout)+' | batchsize = '+str(batchsize))\n",
    "    \n",
    "    model.fit(xtrain ,ytrain, batch_size = batchsize,epochs = 200, validation_data=(xval,yval),verbose=0,callbacks=cb)\n",
    "    score = 100-100*model.evaluate(xtest,ytest,verbose=0)[1]\n",
    "    print('error rate (%) = '+str(score))\n",
    "    print('---------------------------------------------------------')\n",
    "      \n",
    "    return score\n",
    "\n",
    "def objective_vec(hyper_vec,n):\n",
    "    \n",
    "    res_vec=np.zeros((n,1))\n",
    "    for i in range(n):\n",
    "        res_vec[i]=objective(hyper_vec[i,:])\n",
    "        \n",
    "    return(res_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SWARM DEFINITION\n",
    "\n",
    "## Swarm Initialization\n",
    "# the position and the velocity of n particles in the d dimensional space are initialized randomly\n",
    "def initializeSwarm(n,d,hist):\n",
    "    \n",
    "    position=np.random.rand(n,d)\n",
    "    velocity=np.random.rand(n,d)-0.5\n",
    "    \n",
    "    position_pbest=np.copy(position)  \n",
    "    value_pbest=objective_vec(position_pbest,n)\n",
    "    \n",
    "    position_gbest=position[value_pbest.argmax(axis=0),:]\n",
    "    value_gbest=value_pbest[value_pbest.argmax(axis=0),:]\n",
    "    \n",
    "    for i in range(n):\n",
    "        hist.append([i,np.copy(position_pbest[i,:]),float(np.copy(value_pbest[i])),np.copy(position_pbest[i,:]),float(np.copy(value_pbest[i])),np.copy(position_gbest),float(np.copy(value_gbest)),np.copy(velocity[i,:])])\n",
    " \n",
    "    return position,velocity,position_pbest,value_pbest,position_gbest,value_gbest,hist\n",
    "\n",
    "## Swarm Definition\n",
    "# the n particles move in the search space according to the Swarm update formulas\n",
    "def evolutionSwarm(n,position,velocity,position_pbest,value_pbest,position_gbest,value_gbest,omega,c1,c2,hist):\n",
    "    for i in range(n):\n",
    "        #print('###')\n",
    "        #print('particle '+str(i))\n",
    "        #print('previous position : '+str(position[i]))\n",
    "        r1=np.random.rand()\n",
    "        r2=np.random.rand()\n",
    "        velocity[i,:]=omega*velocity[i,:]+c1*r1*(position_pbest[i,:]-position[i,:])+c2*r2*(position_gbest-position[i,:])\n",
    "        position[i,:]=position[i,:]+velocity[i,:]\n",
    "        position=np.clip(position,0,1)\n",
    "        #print('new position : '+str(position[i]))\n",
    "        value=objective(position[i,:])\n",
    "        #print(value)\n",
    "        if value>value_pbest[i]:\n",
    "            #print('new personal best')\n",
    "            position_pbest[i]=position[i]\n",
    "            value_pbest[i]=value\n",
    "            if value>value_gbest:\n",
    "                position_gbest=position[i]\n",
    "                value_gbest=value\n",
    "                print('### new global best: '+str(value_gbest)+' ###')\n",
    "                print('---------------------------------------------------------')\n",
    "        hist.append([i,np.copy(position[i,:]),float(np.copy(value)),np.copy(position_pbest[i,:]),float(np.copy(value_pbest[i])),np.copy(position_gbest),float(np.copy(value_gbest)),np.copy(velocity[i,:])])        \n",
    "    #print('--- global best = '+str(position_gbest))\n",
    "    return position,velocity,position_pbest,value_pbest,position_gbest,value_gbest,hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SWARM OPTIMIZATION\n",
    "\n",
    "n=5\n",
    "d=3\n",
    "omega=0.5\n",
    "c1=0.5\n",
    "c2=0.5\n",
    "\n",
    "P=[]\n",
    "hist=[]\n",
    "# Initialization\n",
    "np.random.seed(4)\n",
    "position,velocity,position_pbest,value_pbest,position_gbest,value_gbest,hist=initializeSwarm(n,d,hist)\n",
    "P.append(np.copy(position))\n",
    "\n",
    "for t in range(15):\n",
    "    \n",
    "    #print('===================== new iteration ======================')\n",
    "\n",
    "    position,velocity,position_pbest,value_pbest,position_gbest,value_gbest,hist=evolutionSwarm(n,position,velocity,position_pbest,value_pbest,position_gbest,value_gbest,omega,c1,c2,hist)\n",
    "    P.append(np.copy(position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TPE added\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "\n",
    "space = {'l2reg': hp.uniform('l2reg', 0, 1),\n",
    "         'dropout': hp.uniform('dropout', 0, 1),\n",
    "         'batchsize':hp.uniform('batchsizesize', 0, 1)}\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "            space=space, \n",
    "            algo=tpe.suggest,\n",
    "            max_evals=60,trials=trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-env)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
